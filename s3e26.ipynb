{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnkTIm8aebST"
      },
      "outputs": [],
      "source": [
        "!pip install -q scikit-lego\n",
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install tensorflow_decision_forests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor,RandomForestRegressor,BaggingRegressor\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "import missingno as msno\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.colors as mcolors\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.base import clone\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklego.linear_model import LADRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import median_absolute_error, roc_auc_score, roc_curve\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optuna\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "!wget https://raw.githubusercontent.com/h4pZ/rose-pine-matplotlib/main/themes/rose-pine-dawn.mplstyle -P /tmp\n",
        "plt.style.use(\"/tmp/rose-pine-dawn.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c playground-series-s3e26\n",
        "!unzip playground-series-s3e25.zip"
      ],
      "metadata": {
        "id": "UYKWVM3SfrCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv').drop(columns=['id'],axis=1)\n",
        "test = pd.read_csv('test.csv').drop(\"id\",axis=1)\n",
        "sample_submisiion = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "jjeGrgkbvr1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataframe(df):\n",
        "    \"\"\"\n",
        "    Analyze a pandas DataFrame and provide a summary of its characteristics.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"DataFrame Information:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.info(verbose=True, show_counts=True))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Values:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.head(5).T)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Description:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.describe().T)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Null Values:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Duplicated Rows:\")\n",
        "    print(\"--------------------------\")\n",
        "    display(df.duplicated().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Unique Values:\")\n",
        "    print(\"------------------------\")\n",
        "    display(df.nunique())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Shape:\")\n",
        "    print(\"----------------\")\n",
        "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "\n",
        "# Usage\n",
        "analyze_dataframe(train)"
      ],
      "metadata": {
        "id": "C6rk8dBN-5tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df.columns = ['id', 'N_Days', 'Status', 'Drug', 'Age', 'Sex', 'Ascites',\n",
        "       'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol',\n",
        "       'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets',\n",
        "       'Prothrombin', 'Stage']\n",
        "\n",
        "set_frame_style(syn_df.head(),'Synthetically Generated Data\\n')\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Features\",\"Unique Values\"]\n",
        "for i in list(syn_df.columns) :\n",
        "    nunique =syn_df[str(i)].nunique\n",
        "    table.add_row([i, f\"{nunique()}\"])\n",
        "print('Unique values in synthetically generated dataset : \\n')\n",
        "print(table)\n",
        "\n",
        "df = pd.concat([syn_df,original_df], axis =0)\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.sample(frac = 1).reset_index(drop = True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "D9XD77ArBylS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_days_to_years(age_in_days):\n",
        "    days_in_year = 365.25\n",
        "    age_in_years = age_in_days / days_in_year\n",
        "    return age_in_years\n",
        "\n",
        "\n",
        "def add_cols(df):\n",
        "        age = list(df.Age)\n",
        "        age_in_year = []\n",
        "        for i in age :\n",
        "            age_in_year.append(int(convert_days_to_years(i)))\n",
        "        df['Age_in_year'] = pd.Series(age_in_year)\n",
        "        return df\n",
        "df = add_cols(df)\n",
        "original_df = add_cols(original_df)\n",
        "set_frame_style(df.head())\n",
        "\n",
        "threshold_platelets = 150\n",
        "df['thrombocytopenia'] = np.where(df['Platelets'] < threshold_platelets, 1, 0)\n",
        "test_df['thrombocytopenia'] = np.where(test_df['Platelets'] < threshold_platelets, 1, 0)\n",
        "\n",
        "threshold_alk_phos_upper = 147  # Upper limit of normal range\n",
        "threshold_alk_phos_lower = 44   # Lower limit of normal range\n",
        "\n",
        "df['elevated_alk_phos'] = np.where((df['Alk_Phos'] > threshold_alk_phos_upper) | (df['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
        "test_df['elevated_alk_phos'] = np.where((test_df['Alk_Phos'] > threshold_alk_phos_upper) | (test_df['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
        "\n",
        "normal_copper_range = (62, 140)\n",
        "\n",
        "df['normal_copper'] = np.where((df['Copper'] >= normal_copper_range[0]) & (df['Copper'] <= normal_copper_range[1]), 1, 0)\n",
        "test_df['normal_copper'] = np.where((test_df['Copper'] >= normal_copper_range[0]) & (test_df['Copper'] <= normal_copper_range[1]), 1, 0)\n",
        "\n",
        "normal_albumin_range = (3.4, 5.4)\n",
        "\n",
        "df['normal_albumin'] = np.where((df['Albumin'] >= normal_albumin_range[0]) & (df['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
        "\n",
        "test_df['normal_albumin'] = np.where((test_df['Albumin'] >= normal_albumin_range[0]) & (test_df['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
        "\n",
        "\n",
        "normal_bilirubin_range = (0.2, 1.2)\n",
        "\n",
        "df['normal_bilirubin'] = np.where((df['Bilirubin'] >= normal_bilirubin_range[0]) & (df['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
        "test_df['normal_bilirubin'] = np.where((test_df['Bilirubin'] >= normal_bilirubin_range[0]) & (test_df['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)"
      ],
      "metadata": {
        "id": "ynCCvoTehr4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols= ['N_Days',  'Age',  'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
        "       'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\n",
        "test_to_scale = test_df[numeric_cols]\n",
        "train_to_scale = df[numeric_cols]\n",
        "set_frame_style(train_to_scale.head(), 'Features with continuous values')\n",
        "train_to_scale_original = original_df[numeric_cols]"
      ],
      "metadata": {
        "id": "Njfjt6CbAVOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "columns = list(train_to_scale.columns)\n",
        "\n",
        "ultra_light_colors = [\n",
        "\"#F0F8FF\", \"#F6F6F6\", \"#F0FFF0\",  \"#FAFAD2\",  \"#FFE4E1\",  \"#FFF5EE\", \"#F5FFFA\",  \"#F0FFFF\",\"#FFFAF0\",  \"#F8F8FF\"\n",
        "]\n",
        "fig = make_subplots(rows=len(columns), cols=2)\n",
        "count = 0\n",
        "for row in range(int(len(columns))) :\n",
        "    random_col = f\"RGB({random.randint(100, 255)}, {random.randint(100, 255)}, {random.randint(150, 255)})\"\n",
        "    fig.add_trace(go.Violin(y=syn_df[numeric_cols][columns[count]], x0 = columns[count], box_visible=True, line_color='black',\n",
        "                               meanline_visible=True, fillcolor=random_col, opacity=0.6,), row=row + 1, col= 1)\n",
        "    fig.add_trace(go.Violin(y= train_to_scale_original[columns[count]],x0 = columns[count], box_visible=True, line_color='black',\n",
        "                               meanline_visible=True, fillcolor=random_col, opacity=0.6,), row=row + 1, col= 2)\n",
        "\n",
        "\n",
        "    count +=1\n",
        "\n",
        "\n",
        "fig.update_layout(height=2600, width=1000, title_text=\"Feature Distribution in Synthetic (Left) vs Original Dataset (Right)\",showlegend=False,paper_bgcolor= '#F5F5F5')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "A28M-eGBJjX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_frame_style(pd.DataFrame(df.Status.value_counts()), 'Data points per class', '15px')\n",
        "\n",
        "classes = list(df.Status.unique())\n",
        "\n",
        "ultra_light_colors = [\n",
        "\"#F0F8FF\", \"#F6F6F6\", \"#F0FFF0\",  \"#FAFAD2\",  \"#FFE4E1\",  \"#FFF5EE\", \"#F5FFFA\",  \"#F0FFFF\",\"#FFFAF0\",  \"#F8F8FF\"\n",
        "]\n",
        "def col_per_class(col):\n",
        "    fig = go.Figure()\n",
        "    for clas in classes :\n",
        "        fig.add_trace(go.Violin(y = df[col][df['Status']== clas],   box_visible=True,\n",
        "                            meanline_visible=True , x = df['Status'][df['Status'] == clas], name = clas ))\n",
        "        fig.update_layout(title = f'Distribution for {col} for each class', plot_bgcolor = ultra_light_colors[np.random.randint(1,10)],paper_bgcolor= '#F5F5F5', height=400,\n",
        "        width=1000 )\n",
        "    return fig\n",
        "for i in train_to_scale :\n",
        "    fig = col_per_class(i)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "hehd_wSAEQpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlation_matrix = train_to_scale.corr()\n",
        "\n",
        "# Create a heatmap with masked upper triangle\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=.5)\n",
        "plt.title('Correlation Matrix (Lower Triangle)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlfgRB8sEdSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_frame_style(df.head())\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "\n",
        "\n",
        "scaled_train = pd.DataFrame(sc.fit_transform(train_to_scale),columns = train_to_scale.columns)\n",
        "scaled_test = pd.DataFrame(sc.transform(test_to_scale),columns = test_to_scale.columns)\n",
        "\n",
        "set_frame_style(scaled_train.head())"
      ],
      "metadata": {
        "id": "crxdHKr9EfTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ohe = df.drop(numeric_cols, axis =1)\n",
        "train_to_ohe = train_ohe.drop(['id','Age_in_year','Status'], axis =1)\n",
        "test_ohe = test_df.drop(numeric_cols, axis =1)\n",
        "test_to_ohe = test_ohe.drop(['id'],axis =1)\n",
        "set_frame_style(train_to_ohe.head())\n",
        "\n",
        "set_frame_style(test_to_ohe.head())"
      ],
      "metadata": {
        "id": "yr_1-7SjElMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_train = pd.get_dummies(train_to_ohe, columns =train_to_ohe.columns )\n",
        "ohe_test  = pd.get_dummies(test_to_ohe, columns = train_to_ohe.columns)\n",
        "ohe_train = ohe_train.replace({True: 1, False: 0})\n",
        "ohe_test = ohe_test.replace({True: 1, False: 0})\n",
        "set_frame_style(ohe_train.head())"
      ],
      "metadata": {
        "id": "pc9K9B2eEofQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_1 = pd.concat([ohe_train, scaled_train], axis =1)\n",
        "test_df = pd.concat([ohe_test, scaled_test], axis =1)\n",
        "set_frame_style(train_df_1.head())"
      ],
      "metadata": {
        "id": "N2Xu1rupEqJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_1.info()\n",
        "\n",
        "train_df_1.shape"
      ],
      "metadata": {
        "id": "6jtEitDvErh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X = train_df_1\n",
        "y = df['Status']\n",
        "\n",
        "# Label encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "\n",
        "# X, y_encoded = tomek_links(X, y_encoded)\n",
        "\n",
        "#XGBoost parameters\n",
        "xgb_params = {'max_depth': 6,\n",
        "          'min_child_weight': 10,\n",
        "          'learning_rate': 0.010009541152584345,\n",
        "          'n_estimators': 1878, 'subsample': 0.47524425009347593,\n",
        "          'colsample_bytree': 0.3292032860985591, 'random_state': 42,\n",
        "\n",
        "         'tree_method': 'hist',\n",
        "        'eval_metric': 'mlogloss',\n",
        "          'device' : 'cuda',\n",
        "        'verbosity': 2}\n",
        "\n",
        "# xgb_params = {'max_depth': 5,\n",
        "#  'min_child_weight': 8,\n",
        "#  'learning_rate': 0.10450346600896168,\n",
        "#  'n_estimators': 225,\n",
        "#  'subsample': 0.5855025206558809,\n",
        "#  'colsample_bytree': 0.14926372575849994,\n",
        "#  'reg_alpha': 0.7621405624015435,\n",
        "#  'reg_lambda': 0.6443164876665903,\n",
        "#          'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,\n",
        "#  'random_state': 42}\n",
        "\n",
        "# xgb_params ={'max_depth': 10,\n",
        "#          'min_child_weight': 7,\n",
        "#          'learning_rate': 0.03419253503641095,\n",
        "#          'n_estimators': 472,\n",
        "#          'subsample': 0.8843005833909504,\n",
        "#          'colsample_bytree': 0.0966352677605082,\n",
        "#          'random_state': 42,\n",
        "#          'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2, }\n",
        "\n",
        "# xgb_params ={'max_depth': 8,\n",
        "#  'min_child_weight': 1,\n",
        "#  'learning_rate': 0.04242597466435193,\n",
        "#  'n_estimators': 686,\n",
        "#  'subsample': 0.5999261787239216,\n",
        "#  'colsample_bytree': 0.055073941529892194,\n",
        "#  'random_state': 42,\n",
        "# 'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,}\n",
        "\n",
        "# xgb_params = {\n",
        "#     'max_depth': 8,\n",
        "#     'min_child_weight': 6,\n",
        "#     'learning_rate': 0.013401933024622273,\n",
        "#     'n_estimators': 1472,\n",
        "#     'subsample': 0.281863681376234,\n",
        "#     'colsample_bytree': 0.15983733796975869,\n",
        "#     'random_state': 42,\n",
        "#     'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,\n",
        "# }\n",
        "\n",
        "# number of folds\n",
        "n_splits = 10\n",
        "\n",
        "#  StratifiedKFold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "#  cross-validation results\n",
        "cv_results = []\n",
        "\n",
        "# stratified k-fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = XGBClassifier(**xgb_params )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train )\n",
        "\n",
        "    # predictions on the validation set\n",
        "    y_val_pred_prob = xgb_model.predict_proba(X_val)\n",
        "\n",
        "    # Evaluating the model\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "    # results\n",
        "    cv_results.append(logloss)\n",
        "\n",
        "# average cross-validation result\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')\n"
      ],
      "metadata": {
        "id": "3A1gaotAExon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "feature_names = train_df_1.columns\n",
        "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_feature_names, sorted_importance_scores = zip(*sorted_feature_importance)\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.barh(sorted_feature_names, sorted_importance_scores)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jbR6Ef_6EyMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import log_loss\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# parameters\n",
        "# lgbm_params = {\n",
        "# 'n_estimators': 479,\n",
        "#  'learning_rate': 0.011780201673685327,\n",
        "#  'min_child_weight': 3.100349976889409,\n",
        "#  'subsample': 0.4975668919008305,\n",
        "#  'subsample_freq': 0,\n",
        "#  'colsample_bytree': 0.3960164652068705,\n",
        "#     'random_state': 42,\n",
        "#     'objective': 'multiclass',\n",
        "#     'num_class': 3,  # Number of classes in your target variable\n",
        "#     'metric': 'multi_logloss',\n",
        "#     'device': 'gpu',\n",
        "#     'verbosity': 0\n",
        "# }\n",
        "lgbm_params = {'objective': 'multi_logloss',\n",
        "               'max_depth': 9, 'min_child_samples': 14,\n",
        "               'learning_rate': 0.034869481921747415,\n",
        "               'n_estimators': 274, 'min_child_weight': 9,\n",
        "               'subsample': 0.7717873512945741,\n",
        "               'colsample_bytree': 0.1702910221565107,\n",
        "               'reg_alpha': 0.10626128775335533,\n",
        "               'reg_lambda': 0.624196407787772,\n",
        "               'random_state': 42,\n",
        "\n",
        "\n",
        "              }\n",
        "\n",
        "\n",
        "# folds\n",
        "n_splits = 10\n",
        "\n",
        "# StratifiedKFold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = []\n",
        "\n",
        "\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    lgbm_model = LGBMClassifier(**lgbm_params)\n",
        "    lgbm_model.fit(X_train,y_train)\n",
        "\n",
        "    y_val_pred_prob = lgbm_model.predict_proba(X_val)\n",
        "\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "    cv_results.append(logloss)\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')"
      ],
      "metadata": {
        "id": "jDZ4uoZSE4bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "catboost_params = {'iterations': 469,\n",
        "                   'depth': 20,\n",
        "                   'min_data_in_leaf': 11,\n",
        "                   'learning_rate': 0.13812945166006543,\n",
        "                   'grow_policy': 'Lossguide',\n",
        "                   'bootstrap_type' : 'Bernoulli'}\n",
        "\n",
        "n_splits = 10\n",
        "\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "cv_results = []"
      ],
      "metadata": {
        "id": "crz2qA1YqeY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    cat_model = CatBoostClassifier(**catboost_params,\n",
        "                            random_state=42, verbose =0\n",
        "                           )\n",
        "    cat_model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "    y_val_pred_prob = cat_model.predict_proba(X_val)\n",
        "\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "\n",
        "    cv_results.append(logloss)\n",
        "\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')"
      ],
      "metadata": {
        "id": "ULhgh4XjE7UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "# lgb_1 = LGBMClassifier(**lgbm_params )\n",
        "# xgb_1 = XGBClassifier(**xgb_params )\n",
        "# cb_1 = CatBoostClassifier(**catboost_params, random_state=42)\n",
        "Ensemble = VotingClassifier(estimators = [('lgb', lgbm_model), ('xgb', xgb_model), ('CB', cat_model)],\n",
        "                            voting='soft',\n",
        "                            weights = [0.45,0.5,0.05]   #Adjust weighting since XGB performs better in local environment\n",
        "                            )\n",
        "Ensemble.fit(X, y_encoded)"
      ],
      "metadata": {
        "id": "pJ_0O1BkE-Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "feature_names = train_df_1.columns\n",
        "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_feature_names, sorted_importance_scores = zip(*sorted_feature_importance)\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.barh(sorted_feature_names, sorted_importance_scores)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "B5AjeE7UFCgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Ensemble.predict_proba(test_df)\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "y_pred.columns = ['Status_C', 'Status_CL','Status_D']\n",
        "y_pred.head()\n",
        "\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df = y_pred\n",
        "submission_df['id'] = ids\n",
        "submission_df.head()\n",
        "\n",
        "submission_df.to_csv('submission.csv', index= False)"
      ],
      "metadata": {
        "id": "-BakJ8I0FGLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}