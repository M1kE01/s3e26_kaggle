{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j1J-CBrHVfJQ",
        "35mACr41VlSU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnkTIm8aebST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7b1443-c948-4c5c-8bf2-2ced111bddf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: tensorflow_decision_forests in /usr/local/lib/python3.10/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.5.3)\n",
            "Requirement already satisfied: tensorflow~=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (2.15.0.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (0.42.0)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.10/dist-packages (from tensorflow_decision_forests) (3.0.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tensorflow_decision_forests) (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow_decision_forests) (2023.3.post1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tensorflow_decision_forests) (3.2.2)\n",
            "--2023-12-14 11:26:24--  https://raw.githubusercontent.com/h4pZ/rose-pine-matplotlib/main/themes/rose-pine-dawn.mplstyle\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40905 (40K) [text/plain]\n",
            "Saving to: ‘/tmp/rose-pine-dawn.mplstyle’\n",
            "\n",
            "rose-pine-dawn.mpls 100%[===================>]  39.95K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-12-14 11:26:24 (12.7 MB/s) - ‘/tmp/rose-pine-dawn.mplstyle’ saved [40905/40905]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q scikit-lego\n",
        "!pip install catboost\n",
        "!pip install xgboost\n",
        "!pip install tensorflow_decision_forests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import median_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor,RandomForestRegressor,BaggingRegressor\n",
        "\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_decision_forests as tfdf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import tensorflow as tf\n",
        "import missingno as msno\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.colors as mcolors\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from sklearn.base import clone\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.spatial.distance import squareform\n",
        "from sklego.linear_model import LADRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, VotingRegressor\n",
        "from sklearn.metrics import median_absolute_error, roc_auc_score, roc_curve\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "!wget https://raw.githubusercontent.com/h4pZ/rose-pine-matplotlib/main/themes/rose-pine-dawn.mplstyle -P /tmp\n",
        "plt.style.use(\"/tmp/rose-pine-dawn.mplstyle\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp '/content/drive/MyDrive/kaggle.json' ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c playground-series-s3e26\n",
        "!unzip playground-series-s3e26.zip"
      ],
      "metadata": {
        "id": "UYKWVM3SfrCK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5489be-586d-4130-d987-829134a03cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading playground-series-s3e26.zip to /content\n",
            "  0% 0.00/350k [00:00<?, ?B/s]\n",
            "100% 350k/350k [00:00<00:00, 71.9MB/s]\n",
            "Archive:  playground-series-s3e26.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First approach"
      ],
      "metadata": {
        "id": "j1J-CBrHVfJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv').drop(columns=['id'],axis=1)\n",
        "test = pd.read_csv('test.csv').drop(\"id\",axis=1)\n",
        "sample_submisiion = pd.read_csv('sample_submission.csv')"
      ],
      "metadata": {
        "id": "jjeGrgkbvr1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_dataframe(df):\n",
        "    \"\"\"\n",
        "    Analyze a pandas DataFrame and provide a summary of its characteristics.\n",
        "\n",
        "    Parameters:\n",
        "    df (pandas.DataFrame): The input DataFrame to analyze.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"DataFrame Information:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.info(verbose=True, show_counts=True))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Values:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.head(5).T)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Description:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.describe().T)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Null Values:\")\n",
        "    print(\"----------------------\")\n",
        "    display(df.isnull().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Duplicated Rows:\")\n",
        "    print(\"--------------------------\")\n",
        "    display(df.duplicated().sum())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"Number of Unique Values:\")\n",
        "    print(\"------------------------\")\n",
        "    display(df.nunique())\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\"DataFrame Shape:\")\n",
        "    print(\"----------------\")\n",
        "    print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "\n",
        "# Usage\n",
        "analyze_dataframe(train)"
      ],
      "metadata": {
        "id": "C6rk8dBN-5tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_df.columns = ['id', 'N_Days', 'Status', 'Drug', 'Age', 'Sex', 'Ascites',\n",
        "       'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol',\n",
        "       'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets',\n",
        "       'Prothrombin', 'Stage']\n",
        "\n",
        "set_frame_style(syn_df.head(),'Synthetically Generated Data\\n')\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Features\",\"Unique Values\"]\n",
        "for i in list(syn_df.columns) :\n",
        "    nunique =syn_df[str(i)].nunique\n",
        "    table.add_row([i, f\"{nunique()}\"])\n",
        "print('Unique values in synthetically generated dataset : \\n')\n",
        "print(table)\n",
        "\n",
        "df = pd.concat([syn_df,original_df], axis =0)\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.sample(frac = 1).reset_index(drop = True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "D9XD77ArBylS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_days_to_years(age_in_days):\n",
        "    days_in_year = 365.25\n",
        "    age_in_years = age_in_days / days_in_year\n",
        "    return age_in_years\n",
        "\n",
        "\n",
        "def add_cols(df):\n",
        "        age = list(df.Age)\n",
        "        age_in_year = []\n",
        "        for i in age :\n",
        "            age_in_year.append(int(convert_days_to_years(i)))\n",
        "        df['Age_in_year'] = pd.Series(age_in_year)\n",
        "        return df\n",
        "df = add_cols(df)\n",
        "original_df = add_cols(original_df)\n",
        "set_frame_style(df.head())\n",
        "\n",
        "threshold_platelets = 150\n",
        "df['thrombocytopenia'] = np.where(df['Platelets'] < threshold_platelets, 1, 0)\n",
        "test_df['thrombocytopenia'] = np.where(test_df['Platelets'] < threshold_platelets, 1, 0)\n",
        "\n",
        "threshold_alk_phos_upper = 147  # Upper limit of normal range\n",
        "threshold_alk_phos_lower = 44   # Lower limit of normal range\n",
        "\n",
        "df['elevated_alk_phos'] = np.where((df['Alk_Phos'] > threshold_alk_phos_upper) | (df['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
        "test_df['elevated_alk_phos'] = np.where((test_df['Alk_Phos'] > threshold_alk_phos_upper) | (test_df['Alk_Phos'] < threshold_alk_phos_lower), 1, 0)\n",
        "\n",
        "normal_copper_range = (62, 140)\n",
        "\n",
        "df['normal_copper'] = np.where((df['Copper'] >= normal_copper_range[0]) & (df['Copper'] <= normal_copper_range[1]), 1, 0)\n",
        "test_df['normal_copper'] = np.where((test_df['Copper'] >= normal_copper_range[0]) & (test_df['Copper'] <= normal_copper_range[1]), 1, 0)\n",
        "\n",
        "normal_albumin_range = (3.4, 5.4)\n",
        "\n",
        "df['normal_albumin'] = np.where((df['Albumin'] >= normal_albumin_range[0]) & (df['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
        "\n",
        "test_df['normal_albumin'] = np.where((test_df['Albumin'] >= normal_albumin_range[0]) & (test_df['Albumin'] <= normal_albumin_range[1]), 1, 0)\n",
        "\n",
        "\n",
        "normal_bilirubin_range = (0.2, 1.2)\n",
        "\n",
        "df['normal_bilirubin'] = np.where((df['Bilirubin'] >= normal_bilirubin_range[0]) & (df['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)\n",
        "test_df['normal_bilirubin'] = np.where((test_df['Bilirubin'] >= normal_bilirubin_range[0]) & (test_df['Bilirubin'] <= normal_bilirubin_range[1]), 1, 0)"
      ],
      "metadata": {
        "id": "ynCCvoTehr4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols= ['N_Days',  'Age',  'Bilirubin', 'Cholesterol', 'Albumin', 'Copper',\n",
        "       'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin']\n",
        "test_to_scale = test_df[numeric_cols]\n",
        "train_to_scale = df[numeric_cols]\n",
        "set_frame_style(train_to_scale.head(), 'Features with continuous values')\n",
        "train_to_scale_original = original_df[numeric_cols]"
      ],
      "metadata": {
        "id": "Njfjt6CbAVOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "columns = list(train_to_scale.columns)\n",
        "\n",
        "ultra_light_colors = [\n",
        "\"#F0F8FF\", \"#F6F6F6\", \"#F0FFF0\",  \"#FAFAD2\",  \"#FFE4E1\",  \"#FFF5EE\", \"#F5FFFA\",  \"#F0FFFF\",\"#FFFAF0\",  \"#F8F8FF\"\n",
        "]\n",
        "fig = make_subplots(rows=len(columns), cols=2)\n",
        "count = 0\n",
        "for row in range(int(len(columns))) :\n",
        "    random_col = f\"RGB({random.randint(100, 255)}, {random.randint(100, 255)}, {random.randint(150, 255)})\"\n",
        "    fig.add_trace(go.Violin(y=syn_df[numeric_cols][columns[count]], x0 = columns[count], box_visible=True, line_color='black',\n",
        "                               meanline_visible=True, fillcolor=random_col, opacity=0.6,), row=row + 1, col= 1)\n",
        "    fig.add_trace(go.Violin(y= train_to_scale_original[columns[count]],x0 = columns[count], box_visible=True, line_color='black',\n",
        "                               meanline_visible=True, fillcolor=random_col, opacity=0.6,), row=row + 1, col= 2)\n",
        "\n",
        "\n",
        "    count +=1\n",
        "\n",
        "\n",
        "fig.update_layout(height=2600, width=1000, title_text=\"Feature Distribution in Synthetic (Left) vs Original Dataset (Right)\",showlegend=False,paper_bgcolor= '#F5F5F5')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "A28M-eGBJjX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_frame_style(pd.DataFrame(df.Status.value_counts()), 'Data points per class', '15px')\n",
        "\n",
        "classes = list(df.Status.unique())\n",
        "\n",
        "ultra_light_colors = [\n",
        "\"#F0F8FF\", \"#F6F6F6\", \"#F0FFF0\",  \"#FAFAD2\",  \"#FFE4E1\",  \"#FFF5EE\", \"#F5FFFA\",  \"#F0FFFF\",\"#FFFAF0\",  \"#F8F8FF\"\n",
        "]\n",
        "def col_per_class(col):\n",
        "    fig = go.Figure()\n",
        "    for clas in classes :\n",
        "        fig.add_trace(go.Violin(y = df[col][df['Status']== clas],   box_visible=True,\n",
        "                            meanline_visible=True , x = df['Status'][df['Status'] == clas], name = clas ))\n",
        "        fig.update_layout(title = f'Distribution for {col} for each class', plot_bgcolor = ultra_light_colors[np.random.randint(1,10)],paper_bgcolor= '#F5F5F5', height=400,\n",
        "        width=1000 )\n",
        "    return fig\n",
        "for i in train_to_scale :\n",
        "    fig = col_per_class(i)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "hehd_wSAEQpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "correlation_matrix = train_to_scale.corr()\n",
        "\n",
        "# Create a heatmap with masked upper triangle\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='Blues', fmt='.2f', linewidths=.5)\n",
        "plt.title('Correlation Matrix (Lower Triangle)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlfgRB8sEdSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_frame_style(df.head())\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "sc = MinMaxScaler()\n",
        "\n",
        "\n",
        "scaled_train = pd.DataFrame(sc.fit_transform(train_to_scale),columns = train_to_scale.columns)\n",
        "scaled_test = pd.DataFrame(sc.transform(test_to_scale),columns = test_to_scale.columns)\n",
        "\n",
        "set_frame_style(scaled_train.head())"
      ],
      "metadata": {
        "id": "crxdHKr9EfTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ohe = df.drop(numeric_cols, axis =1)\n",
        "train_to_ohe = train_ohe.drop(['id','Age_in_year','Status'], axis =1)\n",
        "test_ohe = test_df.drop(numeric_cols, axis =1)\n",
        "test_to_ohe = test_ohe.drop(['id'],axis =1)\n",
        "set_frame_style(train_to_ohe.head())\n",
        "\n",
        "set_frame_style(test_to_ohe.head())"
      ],
      "metadata": {
        "id": "yr_1-7SjElMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_train = pd.get_dummies(train_to_ohe, columns =train_to_ohe.columns )\n",
        "ohe_test  = pd.get_dummies(test_to_ohe, columns = train_to_ohe.columns)\n",
        "ohe_train = ohe_train.replace({True: 1, False: 0})\n",
        "ohe_test = ohe_test.replace({True: 1, False: 0})\n",
        "set_frame_style(ohe_train.head())"
      ],
      "metadata": {
        "id": "pc9K9B2eEofQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_1 = pd.concat([ohe_train, scaled_train], axis =1)\n",
        "test_df = pd.concat([ohe_test, scaled_test], axis =1)\n",
        "set_frame_style(train_df_1.head())"
      ],
      "metadata": {
        "id": "N2Xu1rupEqJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_1.info()\n",
        "\n",
        "train_df_1.shape"
      ],
      "metadata": {
        "id": "6jtEitDvErh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X = train_df_1\n",
        "y = df['Status']\n",
        "\n",
        "# Label encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "\n",
        "\n",
        "# X, y_encoded = tomek_links(X, y_encoded)\n",
        "\n",
        "#XGBoost parameters\n",
        "xgb_params = {'max_depth': 6,\n",
        "          'min_child_weight': 10,\n",
        "          'learning_rate': 0.010009541152584345,\n",
        "          'n_estimators': 1878, 'subsample': 0.47524425009347593,\n",
        "          'colsample_bytree': 0.3292032860985591, 'random_state': 42,\n",
        "\n",
        "         'tree_method': 'hist',\n",
        "        'eval_metric': 'mlogloss',\n",
        "          'device' : 'cuda',\n",
        "        'verbosity': 2}\n",
        "\n",
        "# xgb_params = {'max_depth': 5,\n",
        "#  'min_child_weight': 8,\n",
        "#  'learning_rate': 0.10450346600896168,\n",
        "#  'n_estimators': 225,\n",
        "#  'subsample': 0.5855025206558809,\n",
        "#  'colsample_bytree': 0.14926372575849994,\n",
        "#  'reg_alpha': 0.7621405624015435,\n",
        "#  'reg_lambda': 0.6443164876665903,\n",
        "#          'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,\n",
        "#  'random_state': 42}\n",
        "\n",
        "# xgb_params ={'max_depth': 10,\n",
        "#          'min_child_weight': 7,\n",
        "#          'learning_rate': 0.03419253503641095,\n",
        "#          'n_estimators': 472,\n",
        "#          'subsample': 0.8843005833909504,\n",
        "#          'colsample_bytree': 0.0966352677605082,\n",
        "#          'random_state': 42,\n",
        "#          'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2, }\n",
        "\n",
        "# xgb_params ={'max_depth': 8,\n",
        "#  'min_child_weight': 1,\n",
        "#  'learning_rate': 0.04242597466435193,\n",
        "#  'n_estimators': 686,\n",
        "#  'subsample': 0.5999261787239216,\n",
        "#  'colsample_bytree': 0.055073941529892194,\n",
        "#  'random_state': 42,\n",
        "# 'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,}\n",
        "\n",
        "# xgb_params = {\n",
        "#     'max_depth': 8,\n",
        "#     'min_child_weight': 6,\n",
        "#     'learning_rate': 0.013401933024622273,\n",
        "#     'n_estimators': 1472,\n",
        "#     'subsample': 0.281863681376234,\n",
        "#     'colsample_bytree': 0.15983733796975869,\n",
        "#     'random_state': 42,\n",
        "#     'tree_method': 'hist',\n",
        "#         'eval_metric': 'mlogloss',\n",
        "#           'device' : 'cuda',\n",
        "#         'verbosity': 2,\n",
        "# }\n",
        "\n",
        "# number of folds\n",
        "n_splits = 10\n",
        "\n",
        "#  StratifiedKFold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "#  cross-validation results\n",
        "cv_results = []\n",
        "\n",
        "# stratified k-fold cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "\n",
        "    # XGBoost model\n",
        "    xgb_model = XGBClassifier(**xgb_params )\n",
        "\n",
        "    xgb_model.fit(X_train, y_train )\n",
        "\n",
        "    # predictions on the validation set\n",
        "    y_val_pred_prob = xgb_model.predict_proba(X_val)\n",
        "\n",
        "    # Evaluating the model\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "    # results\n",
        "    cv_results.append(logloss)\n",
        "\n",
        "# average cross-validation result\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')\n"
      ],
      "metadata": {
        "id": "3A1gaotAExon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "feature_names = train_df_1.columns\n",
        "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_feature_names, sorted_importance_scores = zip(*sorted_feature_importance)\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.barh(sorted_feature_names, sorted_importance_scores)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jbR6Ef_6EyMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import log_loss\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# parameters\n",
        "# lgbm_params = {\n",
        "# 'n_estimators': 479,\n",
        "#  'learning_rate': 0.011780201673685327,\n",
        "#  'min_child_weight': 3.100349976889409,\n",
        "#  'subsample': 0.4975668919008305,\n",
        "#  'subsample_freq': 0,\n",
        "#  'colsample_bytree': 0.3960164652068705,\n",
        "#     'random_state': 42,\n",
        "#     'objective': 'multiclass',\n",
        "#     'num_class': 3,  # Number of classes in your target variable\n",
        "#     'metric': 'multi_logloss',\n",
        "#     'device': 'gpu',\n",
        "#     'verbosity': 0\n",
        "# }\n",
        "lgbm_params = {'objective': 'multi_logloss',\n",
        "               'max_depth': 9, 'min_child_samples': 14,\n",
        "               'learning_rate': 0.034869481921747415,\n",
        "               'n_estimators': 274, 'min_child_weight': 9,\n",
        "               'subsample': 0.7717873512945741,\n",
        "               'colsample_bytree': 0.1702910221565107,\n",
        "               'reg_alpha': 0.10626128775335533,\n",
        "               'reg_lambda': 0.624196407787772,\n",
        "               'random_state': 42,\n",
        "\n",
        "\n",
        "              }\n",
        "\n",
        "\n",
        "# folds\n",
        "n_splits = 10\n",
        "\n",
        "# StratifiedKFold\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "cv_results = []\n",
        "\n",
        "\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    lgbm_model = LGBMClassifier(**lgbm_params)\n",
        "    lgbm_model.fit(X_train,y_train)\n",
        "\n",
        "    y_val_pred_prob = lgbm_model.predict_proba(X_val)\n",
        "\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "    cv_results.append(logloss)\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')"
      ],
      "metadata": {
        "id": "jDZ4uoZSE4bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "catboost_params = {'iterations': 469,\n",
        "                   'depth': 20,\n",
        "                   'min_data_in_leaf': 11,\n",
        "                   'learning_rate': 0.13812945166006543,\n",
        "                   'grow_policy': 'Lossguide',\n",
        "                   'bootstrap_type' : 'Bernoulli'}\n",
        "\n",
        "n_splits = 10\n",
        "\n",
        "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "cv_results = []"
      ],
      "metadata": {
        "id": "crz2qA1YqeY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y_encoded)):\n",
        "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    cat_model = CatBoostClassifier(**catboost_params,\n",
        "                            random_state=42, verbose =0\n",
        "                           )\n",
        "    cat_model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "    y_val_pred_prob = cat_model.predict_proba(X_val)\n",
        "\n",
        "    logloss = log_loss(y_val, y_val_pred_prob)\n",
        "    print(f'Fold {fold + 1}, Logarithmic Loss on Validation Set: {logloss}')\n",
        "\n",
        "\n",
        "    cv_results.append(logloss)\n",
        "\n",
        "average_cv_result = sum(cv_results) / n_splits\n",
        "print(f'\\nAverage Logarithmic Loss across {n_splits} folds: {average_cv_result}')"
      ],
      "metadata": {
        "id": "ULhgh4XjE7UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "# lgb_1 = LGBMClassifier(**lgbm_params )\n",
        "# xgb_1 = XGBClassifier(**xgb_params )\n",
        "# cb_1 = CatBoostClassifier(**catboost_params, random_state=42)\n",
        "Ensemble = VotingClassifier(estimators = [('lgb', lgbm_model), ('xgb', xgb_model), ('CB', cat_model)],\n",
        "                            voting='soft',\n",
        "                            weights = [0.45,0.5,0.05]   #Adjust weighting since XGB performs better in local environment\n",
        "                            )\n",
        "Ensemble.fit(X, y_encoded)"
      ],
      "metadata": {
        "id": "pJ_0O1BkE-Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "feature_importances = xgb_model.feature_importances_\n",
        "feature_names = train_df_1.columns\n",
        "feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_feature_names, sorted_importance_scores = zip(*sorted_feature_importance)\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "plt.barh(sorted_feature_names, sorted_importance_scores)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature Name\")\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "test_df.info()"
      ],
      "metadata": {
        "id": "B5AjeE7UFCgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Ensemble.predict_proba(test_df)\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "y_pred.columns = ['Status_C', 'Status_CL','Status_D']\n",
        "y_pred.head()\n",
        "\n",
        "submission_df = pd.DataFrame()\n",
        "submission_df = y_pred\n",
        "submission_df['id'] = ids\n",
        "submission_df.head()\n",
        "\n",
        "submission_df.to_csv('submission.csv', index= False)"
      ],
      "metadata": {
        "id": "-BakJ8I0FGLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second approach"
      ],
      "metadata": {
        "id": "35mACr41VlSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv('sample_submission.csv')\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "train.info()"
      ],
      "metadata": {
        "id": "X4-WW6f4VnLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc00cb3e-2664-450b-c454-a7d25963b530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7905 entries, 0 to 7904\n",
            "Data columns (total 20 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   id             7905 non-null   int64  \n",
            " 1   N_Days         7905 non-null   int64  \n",
            " 2   Drug           7905 non-null   object \n",
            " 3   Age            7905 non-null   int64  \n",
            " 4   Sex            7905 non-null   object \n",
            " 5   Ascites        7905 non-null   object \n",
            " 6   Hepatomegaly   7905 non-null   object \n",
            " 7   Spiders        7905 non-null   object \n",
            " 8   Edema          7905 non-null   object \n",
            " 9   Bilirubin      7905 non-null   float64\n",
            " 10  Cholesterol    7905 non-null   float64\n",
            " 11  Albumin        7905 non-null   float64\n",
            " 12  Copper         7905 non-null   float64\n",
            " 13  Alk_Phos       7905 non-null   float64\n",
            " 14  SGOT           7905 non-null   float64\n",
            " 15  Tryglicerides  7905 non-null   float64\n",
            " 16  Platelets      7905 non-null   float64\n",
            " 17  Prothrombin    7905 non-null   float64\n",
            " 18  Stage          7905 non-null   float64\n",
            " 19  Status         7905 non-null   object \n",
            "dtypes: float64(10), int64(3), object(7)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis = 1)\n",
        "test = test.drop('id', axis = 1)\n",
        "\n",
        "print(train.isna().sum().any(),  test.isna().sum().any())\n",
        "\n",
        "total = pd.concat([train.drop('Status', axis=1), test], axis = 0)\n",
        "total.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gIfW1jNFiJ9",
        "outputId": "621b0429-f807-458c-d9ec-49a84fb56181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[train.columns].nunique().sort_values(ascending=True)"
      ],
      "metadata": {
        "id": "f3puu94QGBug",
        "outputId": "879ca782-b964-44fa-c929-ea0593849287",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Drug               2\n",
              "Sex                2\n",
              "Ascites            2\n",
              "Hepatomegaly       2\n",
              "Spiders            2\n",
              "Status             3\n",
              "Edema              3\n",
              "Stage              4\n",
              "Prothrombin       49\n",
              "Bilirubin        111\n",
              "Tryglicerides    154\n",
              "Albumin          160\n",
              "Copper           171\n",
              "SGOT             206\n",
              "Cholesterol      226\n",
              "Platelets        227\n",
              "Alk_Phos         364\n",
              "Age              391\n",
              "N_Days           461\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = train.select_dtypes(include = ['int', 'float'])\n",
        "num_train.head()"
      ],
      "metadata": {
        "id": "uiNyIno8qJDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the numerical data\n",
        "rows = (len(num_train.columns)+1)//2\n",
        "_, axes = plt.subplots(rows, 2, figsize = (8, rows*4))\n",
        "for i, col in enumerate(num_train.columns):\n",
        "    hor = i%2\n",
        "    ver = i//2\n",
        "    sns.histplot(data = num_train[col], kde = True, ax = axes[ver, hor])"
      ],
      "metadata": {
        "id": "dS6w51cFqON6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the categorical features\n",
        "cat_train = train.select_dtypes(include = ['object']).astype('category')\n",
        "cat_train.head()"
      ],
      "metadata": {
        "id": "v72eOyDcqUlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the categorical data\n",
        "rows = (len(cat_train.columns)+1)//2\n",
        "_, axes = plt.subplots(rows, 2, figsize = (8, rows*4))\n",
        "for i, col in enumerate(cat_train.columns):\n",
        "    hor = i%2\n",
        "    ver = i//2\n",
        "    sns.histplot(data = cat_train[col], ax = axes[ver, hor])"
      ],
      "metadata": {
        "id": "FJX3NB5bqWRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng = train.copy()\n",
        "test_eng = test.copy()"
      ],
      "metadata": {
        "id": "_J6e1jxaqYsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature engineering for categorical features\n",
        "def col_transform (df:pd.DataFrame):\n",
        "    df_trans  = df.copy()\n",
        "    #apply manual transformation for bionomial features\n",
        "    for col in df.select_dtypes(include=['object']):\n",
        "        if df_trans[col].nunique() <3 and (col != 'Sex'):\n",
        "            if col == 'Drug':\n",
        "                df_trans[col] = df_trans[col].map({'D-penicillamine': True, 'Placebo': False}).astype('bool')\n",
        "            else:\n",
        "                df_trans[col] = df_trans[col].map({'Y': True, 'N': False}).astype('bool')\n",
        "        else:\n",
        "            #apply one hot encoding for others\n",
        "            df_trans = pd.concat([df_trans, pd.get_dummies(df_trans[col], prefix = col)], axis = 1)\n",
        "            df_trans = df_trans.drop(col, axis = 1)\n",
        "    return df_trans"
      ],
      "metadata": {
        "id": "93Si9ui9qbWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng1 =col_transform(train_eng)\n",
        "print(train_eng1)\n",
        "\n",
        "#Apply PowerTransformer for a normalized like distribution\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "num_train3 = num_train.copy()\n",
        "\n",
        "transformer = PowerTransformer(method = 'yeo-johnson')\n",
        "\n",
        "num_train3 = transformer.fit_transform(num_train3)\n",
        "\n",
        "num_train3 = pd.DataFrame(num_train3, columns = num_train.columns)\n",
        "num_train3"
      ],
      "metadata": {
        "id": "Mo1ty0haqc6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = (len(num_train3.columns)+1)//2\n",
        "_, axes = plt.subplots(rows, 2, figsize = (8, rows*4))\n",
        "for i, col in enumerate(num_train3.columns):\n",
        "    hor = i%2\n",
        "    ver = i//2\n",
        "    sns.histplot(data = num_train3[col], ax = axes[ver, hor])"
      ],
      "metadata": {
        "id": "2Q_GYiFFqiWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_engineered = pd.concat([num_train3, train_eng1.drop(num_train3.columns, axis = 1)], axis = 1)\n",
        "train_engineered.head()"
      ],
      "metadata": {
        "id": "nBeqzcECqk1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_test = test_eng.select_dtypes(include=['int', 'float'])\n",
        "test_eng1 = col_transform(test_eng)\n",
        "num_test1 = pd.DataFrame(transformer.transform(num_test), columns = num_test.columns)\n",
        "test_engineered = pd.concat([num_test1, test_eng1.drop(num_test1.columns, axis = 1)], axis = 1)\n",
        "test_engineered.head()"
      ],
      "metadata": {
        "id": "pI6UhuvIqmmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "coder = LabelEncoder()\n",
        "y = coder.fit_transform (train.Status)\n",
        "\n",
        "\n",
        "X = train_engineered.drop(['Status_C', 'Status_CL', 'Status_D'] ,axis = 1)\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "SU9EyjBpqo_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model building\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import cross_val_score\n",
        "lgbmodel = LGBMClassifier(random_state=42, num_class = 3)\n",
        "\n",
        "lgbmodel.fit(X, y)\n",
        "\n",
        "pred = lgbmodel.predict_proba(X)\n",
        "log_loss(y, pred)\n",
        "\n",
        "print (\"CV score of LGBM is \", abs(cross_val_score(lgbmodel, X, y, cv = 4, scoring='neg_log_loss').mean()))\n",
        "\n",
        "rfmodel = RandomForestClassifier(criterion = 'log_loss', random_state=42)\n",
        "print (\"CV score of RFmodel is \", abs(cross_val_score(rfmodel, X, y, cv = 4, scoring='neg_log_loss').mean()))\n",
        "xgbmodel = XGBClassifier(random_state=42)\n",
        "print (\"CV score of xgbmodel is \", abs(cross_val_score(xgbmodel, X, y, cv = 4, scoring='neg_log_loss').mean()))"
      ],
      "metadata": {
        "id": "W6UFGS0Tqshc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third approach"
      ],
      "metadata": {
        "id": "rbUNe0TkNMs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "import concurrent.futures\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import random\n",
        "from random import randint, uniform\n",
        "import gc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xg\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install yellowbrick\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "!pip install gap-stat\n",
        "from gap_statistic.optimalK import OptimalK\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import boxcox\n",
        "import math\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install cmaes\n",
        "import cmaes\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import Pool\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qaUHbGUdNOeV",
        "outputId": "d53d9ec3-ed2c-48a0-bb34-0e9401f647ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yellowbrick in /usr/local/lib/python3.10/dist-packages (1.5)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (1.23.5)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from yellowbrick) (0.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->yellowbrick) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
            "Collecting gap-stat\n",
            "  Downloading gap-stat-2.0.3.tar.gz (17 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gap-stat) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gap-stat) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gap-stat) (1.11.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gap-stat) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gap-stat) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gap-stat) (1.16.0)\n",
            "Building wheels for collected packages: gap-stat\n",
            "  Building wheel for gap-stat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gap-stat: filename=gap_stat-2.0.3-py3-none-any.whl size=6133 sha256=fe5b69cec7c2c37d8b84d1609ee3cd4f6d26133233ba09f5a0de973d0aafab73\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/75/de/ee29b366258cdeccdacaff94d895b9d2ffc95a486f3b982441\n",
            "Successfully built gap-stat\n",
            "Installing collected packages: gap-stat\n",
            "Successfully installed gap-stat-2.0.3\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.0 colorlog-6.8.0 optuna-3.5.0\n",
            "Collecting cmaes\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cmaes) (1.23.5)\n",
            "Installing collected packages: cmaes\n",
            "Successfully installed cmaes-0.10.0\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.2\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: --install-option\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.0)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "Installing collected packages: imbalanced-learn\n",
            "  Attempting uninstall: imbalanced-learn\n",
            "    Found existing installation: imbalanced-learn 0.10.1\n",
            "    Uninstalling imbalanced-learn-0.10.1:\n",
            "      Successfully uninstalled imbalanced-learn-0.10.1\n",
            "Successfully installed imbalanced-learn-0.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "imblearn"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "original=pd.read_csv(\"cirrhosis.csv\")\n",
        "\n",
        "train.drop(columns=[\"id\"],inplace=True)\n",
        "test.drop(columns=[\"id\"],inplace=True)\n",
        "original.drop(columns=[\"ID\"],inplace=True)\n",
        "\n",
        "train_copy=train.copy()\n",
        "test_copy=test.copy()\n",
        "original_copy=original.copy()\n",
        "\n",
        "# cols=[f for f in train.columns if train[f].isna().sum()==0 and original[f].isna().sum()>0]\n",
        "# original=original.loc[original[cols].dropna().index.tolist()]\n",
        "# print(original.shape)\n",
        "\n",
        "original[\"original\"]=1\n",
        "\n",
        "train[\"original\"]=0\n",
        "test[\"original\"]=0\n",
        "\n",
        "train=pd.concat([train,original],axis=0)\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "\n",
        "target='Status'\n",
        "\n",
        "train.head()"
      ],
      "metadata": {
        "id": "w9CqKDFZsdbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "\n",
        "table.field_names = ['Feature', 'Data Type', 'Train Missing %', 'Test Missing %',\"Original Missing%\"]\n",
        "for column in train_copy.columns:\n",
        "    data_type = str(train_copy[column].dtype)\n",
        "    non_null_count_train= np.round(100-train_copy[column].count()/train_copy.shape[0]*100,1)\n",
        "    if column!=target:\n",
        "        non_null_count_test = np.round(100-test_copy[column].count()/test_copy.shape[0]*100,1)\n",
        "    else:\n",
        "        non_null_count_test=\"NA\"\n",
        "    non_null_count_orig= np.round(100-original_copy[column].count()/original_copy.shape[0]*100,1)\n",
        "    table.add_row([column, data_type, non_null_count_train,non_null_count_test,non_null_count_orig])\n",
        "print(table)"
      ],
      "metadata": {
        "id": "-8E4mArnsoa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie_chart(data, title, ax):\n",
        "    data_counts = data[target].value_counts()\n",
        "    labels = data_counts.index\n",
        "    sizes = data_counts.values\n",
        "    colors = [(0.8, 0.56, 0.65), 'crimson',  (0.99, 0.8, 0.3)]\n",
        "    explode = (0.1, 0, 0)\n",
        "\n",
        "    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "    ax.axis('equal')\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 6))  # Create three subplots in a row\n",
        "\n",
        "plot_pie_chart(train, \"Train Target Distribution\", axes[0])\n",
        "plot_pie_chart(original, \"Original Target Distribution\", axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AL3w52uxsx8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols = [f for f in train.columns if train[f].dtype != 'O' and train[f].nunique() > 2]\n",
        "n_rows = len(cont_cols)\n",
        "fig, axs = plt.subplots(n_rows, 2, figsize=(12, 4 * n_rows))\n",
        "sns.set_palette([(0.8, 0.56, 0.65), 'crimson',  (0.99, 0.8, 0.3)])\n",
        "\n",
        "for i, col in enumerate(cont_cols):\n",
        "    sns.violinplot(x=target, y=col, data=train_copy, ax=axs[i, 0])\n",
        "    axs[i, 0].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n",
        "    axs[i, 0].set_xlabel('outcome', fontsize=12)\n",
        "    axs[i, 0].set_ylabel(col.title(), fontsize=12)\n",
        "    sns.despine()\n",
        "\n",
        "    sns.violinplot(x=target, y=col, data=original, ax=axs[i, 1])\n",
        "    axs[i, 1].set_title(f'{col.title()} Distribution by Target (Original)', fontsize=12)\n",
        "    axs[i, 1].set_xlabel('outcome', fontsize=12)\n",
        "    axs[i, 1].set_ylabel(col.title(), fontsize=12)\n",
        "    sns.despine()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UAjrm0ansybF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in train.columns if (train[f].dtype != 'O' and train[f].nunique() / train.shape[0] < 0.025) or (train[f].dtype == 'O' and f not in [target]) ]\n",
        "custom_palette =(0.8, 0.56, 0.65), 'crimson',  (0.99, 0.8, 0.3)\n",
        "for col in cat_cols:\n",
        "    contingency_table = pd.crosstab(train[col], train[target], normalize='index')\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    contingency_table.plot(kind=\"bar\", stacked=True, color=custom_palette,figsize=(20, 4))\n",
        "    plt.title(f\"Percentage Distribution of Target across {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(\"Percentage\")\n",
        "    plt.legend(title=\"Target Class\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qdKZa6bps4Ea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_cat=[f for f in train.columns if train[f].dtype==\"O\" and train[f].isna().sum()>0]\n",
        "train_missing_pct = train[missing_cat].isnull().mean() * 100\n",
        "test_missing_pct = test[missing_cat].isnull().mean() * 100\n",
        "\n",
        "missing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n",
        "print(missing_pct_df)"
      ],
      "metadata": {
        "id": "58-mBa26s65l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params={\n",
        "            'depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'l2_leaf_reg': 0.7,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'MultiClass',\n",
        "            'loss_function': 'MultiClass',\n",
        "}\n",
        "def store_missing_rows(df, features):\n",
        "    missing_rows = {}\n",
        "\n",
        "    for feature in features:\n",
        "        missing_rows[feature] = df[df[feature].isnull()]\n",
        "\n",
        "    return missing_rows\n",
        "\n",
        "def fill_missing_categorical(train, test, target, features, max_iterations=10):\n",
        "    df = pd.concat([train.drop(columns=target), test], axis=\"rows\")\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Step 1: Store the instances with missing values in each feature\n",
        "    missing_rows = store_missing_rows(df, features)\n",
        "\n",
        "    # Step 2: Initially fill all missing values with \"Missing\"\n",
        "    for f in features:\n",
        "        df[f] = df[f].fillna(\"Missing_\" + f)\n",
        "\n",
        "    for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
        "        for feature in features:\n",
        "            # Skip features with no missing values\n",
        "            rows_miss = missing_rows[feature].index\n",
        "\n",
        "            missing_temp = df.loc[rows_miss].copy()\n",
        "            non_missing_temp = df.drop(index=rows_miss).copy()\n",
        "            missing_temp = missing_temp.drop(columns=[feature])\n",
        "\n",
        "            other_features = [x for x in df.columns if x != feature and df[x].dtype == \"O\"]\n",
        "\n",
        "            X_train = non_missing_temp.drop(columns=[feature])\n",
        "            y_train = non_missing_temp[[feature]]\n",
        "\n",
        "            catboost_classifier = CatBoostClassifier(**cat_params)\n",
        "            catboost_classifier.fit(X_train, y_train, cat_features=other_features, verbose=False)\n",
        "\n",
        "            # Step 4: Predict missing values for the feature and update all N features\n",
        "            y_pred = catboost_classifier.predict(missing_temp)\n",
        "\n",
        "            # Convert y_pred to strings if necessary\n",
        "            if y_pred.dtype != \"O\":\n",
        "                y_pred = y_pred.astype(str)\n",
        "\n",
        "            df.loc[rows_miss, feature] = y_pred\n",
        "\n",
        "    train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
        "    test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
        "\n",
        "    return train, test\n",
        "\n",
        "train, test = fill_missing_categorical(train, test, target, missing_cat, 5)"
      ],
      "metadata": {
        "id": "9cRhPYn6s-vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_num=[f for f in train.columns if train[f].dtype!=\"O\" and train[f].isna().sum()>0]\n",
        "train_missing_pct = train[missing_num].isnull().mean() * 100\n",
        "test_missing_pct = test[missing_num].isnull().mean() * 100\n",
        "missing_pct_df = pd.concat([train_missing_pct, test_missing_pct], axis=1, keys=['Train %', 'Test%'])\n",
        "print(missing_pct_df)"
      ],
      "metadata": {
        "id": "Lwtu5POjtAIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_params = {\n",
        "            'iterations': 500,\n",
        "            'depth': 6,\n",
        "            'learning_rate': 0.02,\n",
        "            'l2_leaf_reg': 0.5,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 150,\n",
        "            'od_wait': 80,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'IncToDec',\n",
        "            'eval_metric': 'RMSE',\n",
        "            'loss_function': 'RMSE',\n",
        "            'random_state': 42,\n",
        "        }\n",
        "lgb_params = {\n",
        "            'n_estimators': 50,\n",
        "            'max_depth': 8,\n",
        "            'learning_rate': 0.02,\n",
        "            'subsample': 0.20,\n",
        "            'colsample_bytree': 0.56,\n",
        "            'reg_alpha': 0.25,\n",
        "            'reg_lambda': 5e-08,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_logloss',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'random_state': 42,\n",
        "        }\n",
        "def rmse(y1,y2):\n",
        "    return(np.sqrt(mean_squared_error(y1,y2)))\n",
        "\n",
        "def fill_missing_numerical(train,test,target, features, max_iterations=10):\n",
        "    train_temp=train.copy()\n",
        "    if target in train_temp.columns:\n",
        "        train_temp=train_temp.drop(columns=target)\n",
        "\n",
        "\n",
        "    df=pd.concat([train_temp,test],axis=\"rows\")\n",
        "    df=df.reset_index(drop=True)\n",
        "\n",
        "    # Step 1: Store the instances with missing values in each feature\n",
        "    missing_rows = store_missing_rows(df, features)\n",
        "\n",
        "    # Step 2: Initially fill all missing values with \"Missing\"\n",
        "    for f in features:\n",
        "        df[f]=df[f].fillna(df[f].mean())\n",
        "\n",
        "    cat_features=[f for f in df.columns if not pd.api.types.is_numeric_dtype(df[f])]\n",
        "    dictionary = {feature: [] for feature in features}\n",
        "\n",
        "    for iteration in tqdm(range(max_iterations), desc=\"Iterations\"):\n",
        "        for feature in features:\n",
        "            # Skip features with no missing values\n",
        "            rows_miss = missing_rows[feature].index\n",
        "\n",
        "            missing_temp = df.loc[rows_miss].copy()\n",
        "            non_missing_temp = df.drop(index=rows_miss).copy()\n",
        "            y_pred_prev=missing_temp[feature]\n",
        "            missing_temp = missing_temp.drop(columns=[feature])\n",
        "\n",
        "\n",
        "            # Step 3: Use the remaining features to predict missing values using Random Forests\n",
        "            X_train = non_missing_temp.drop(columns=[feature])\n",
        "            y_train = non_missing_temp[[feature]]\n",
        "\n",
        "            model = CatBoostRegressor(**cb_params)\n",
        "#             if iteration>3:\n",
        "#                 model = lgb.LGBMRegressor()\n",
        "            model.fit(X_train, y_train,cat_features=cat_features, verbose=False)\n",
        "\n",
        "            # Step 4: Predict missing values for the feature and update all N features\n",
        "            y_pred = model.predict(missing_temp)\n",
        "            df.loc[rows_miss, feature] = y_pred\n",
        "            error_minimize=rmse(y_pred,y_pred_prev)\n",
        "            dictionary[feature].append(error_minimize)  # Append the error_minimize value\n",
        "\n",
        "    for feature, values in dictionary.items():\n",
        "        iterations = range(1, len(values) + 1)  # x-axis values (iterations)\n",
        "        plt.plot(iterations, values, label=feature)  # plot the values\n",
        "        plt.xlabel('Iterations')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.title('Minimization of RMSE with iterations')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.show()\n",
        "    train[features] = np.array(df.iloc[:train.shape[0]][features])\n",
        "    test[features] = np.array(df.iloc[train.shape[0]:][features])\n",
        "\n",
        "    return train,test\n",
        "\n",
        "\n",
        "train,test = fill_missing_numerical(train,test,target,missing_num,5)"
      ],
      "metadata": {
        "id": "60XZ-XBYtAVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_map={\n",
        "'C':0,\n",
        "'CL':1,\n",
        "'D':2}\n",
        "\n",
        "\n",
        "def encode(y,target_map):\n",
        "    '''\n",
        "    To convert the outputs to numbers\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    encoded_y=[target_map[f] for f in y]\n",
        "    return encoded_y\n",
        "def decode(y,target_map):\n",
        "    '''To convert the predictions back to classes\n",
        "    '''\n",
        "    y=np.array(y)\n",
        "    reverse_dict={v: k for k, v in target_map.items()}\n",
        "    decoded_y=[reverse_dict[f] for f in y]\n",
        "    return decoded_y\n",
        "def min_max_scaler(train, test, column):\n",
        "    '''\n",
        "    Min Max just based on train might have an issue if test has extreme values, hence changing the denominator uding overall min and max\n",
        "    '''\n",
        "    sc=MinMaxScaler()\n",
        "\n",
        "    max_val=max(train[column].max(),test[column].max())\n",
        "    min_val=min(train[column].min(),test[column].min())\n",
        "\n",
        "    train[column]=(train[column]-min_val)/(max_val-min_val)\n",
        "    test[column]=(test[column]-min_val)/(max_val-min_val)\n",
        "\n",
        "    return train,test\n",
        "\n",
        "def OHE(train_df,test_df,cols,target):\n",
        "    '''\n",
        "    Function for one hot encoding, it first combined the data so that no category is missed and\n",
        "    the category with least frequency can be dropped because of redunancy\n",
        "    '''\n",
        "    combined = pd.concat([train_df, test_df], axis=0)\n",
        "    for col in cols:\n",
        "        one_hot = pd.get_dummies(combined[col])\n",
        "        counts = combined[col].value_counts()\n",
        "        min_count_category = counts.idxmin()\n",
        "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
        "        one_hot.columns=[str(f)+col for f in one_hot.columns]\n",
        "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "    # split back to train and test dataframes\n",
        "    train_ohe = combined[:len(train_df)]\n",
        "    test_ohe = combined[len(train_df):]\n",
        "    test_ohe.reset_index(inplace=True,drop=True)\n",
        "    test_ohe.drop(columns=[target],inplace=True)\n",
        "    return train_ohe, test_ohe"
      ],
      "metadata": {
        "id": "YhQ48MQZtIFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols = [f for f in train.columns if pd.api.types.is_numeric_dtype(train[f]) and train[f].nunique() / train.shape[0] * 100 > 2.5]\n",
        "\n",
        "sc=MinMaxScaler()\n",
        "\n",
        "global unimportant_features\n",
        "global overall_best_score\n",
        "global overall_best_col\n",
        "unimportant_features=[]\n",
        "overall_best_score=100\n",
        "overall_best_col='none'\n",
        "\n",
        "for col in cont_cols:\n",
        "     train, test=min_max_scaler(train, test, col)\n",
        "\n",
        "def transformer(train, test,cont_cols, target):\n",
        "    '''\n",
        "    Algorithm applies multiples transformations on selected columns and finds the best transformation using a single variable model performance\n",
        "    '''\n",
        "    global unimportant_features\n",
        "    global overall_best_score\n",
        "    global overall_best_col\n",
        "    train_copy = train.copy()\n",
        "    test_copy = test.copy()\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Feature', 'Original Log Loss', 'Transformation', 'Tranformed Log Loss']\n",
        "\n",
        "    for col in cont_cols:\n",
        "\n",
        "        for c in [\"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]:\n",
        "            if c in train_copy.columns:\n",
        "                train_copy = train_copy.drop(columns=[c])\n",
        "\n",
        "        # Log Transformation after MinMax Scaling (keeps data between 0 and 1)\n",
        "        train_copy[\"log_\"+col] = np.log1p(train_copy[col])\n",
        "        test_copy[\"log_\"+col] = np.log1p(test_copy[col])\n",
        "\n",
        "        # Square Root Transformation\n",
        "        train_copy[\"sqrt_\"+col] = np.sqrt(train_copy[col])\n",
        "        test_copy[\"sqrt_\"+col] = np.sqrt(test_copy[col])\n",
        "\n",
        "        # Box-Cox transformation\n",
        "        combined_data = pd.concat([train_copy[[col]], test_copy[[col]]], axis=0)\n",
        "        epsilon = 1e-5\n",
        "        transformer = PowerTransformer(method='box-cox')\n",
        "        scaled_data = transformer.fit_transform(combined_data + epsilon)\n",
        "\n",
        "        train_copy[\"bx_cx_\" + col] = scaled_data[:train_copy.shape[0]]\n",
        "        test_copy[\"bx_cx_\" + col] = scaled_data[train_copy.shape[0]:]\n",
        "        # Yeo-Johnson transformation\n",
        "        transformer = PowerTransformer(method='yeo-johnson')\n",
        "        train_copy[\"y_J_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"y_J_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Power transformation, 0.25\n",
        "        power_transform = lambda x: np.power(x + 1 - np.min(x), 0.25)\n",
        "        transformer = FunctionTransformer(power_transform)\n",
        "        train_copy[\"pow_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"pow_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Power transformation, 2\n",
        "        power_transform = lambda x: np.power(x + 1 - np.min(x), 2)\n",
        "        transformer = FunctionTransformer(power_transform)\n",
        "        train_copy[\"pow2_\"+col] = transformer.fit_transform(train_copy[[col]])\n",
        "        test_copy[\"pow2_\"+col] = transformer.transform(test_copy[[col]])\n",
        "\n",
        "        # Log to power transformation\n",
        "        train_copy[\"log_sqrt\"+col] = np.log1p(train_copy[\"sqrt_\"+col])\n",
        "        test_copy[\"log_sqrt\"+col] = np.log1p(test_copy[\"sqrt_\"+col])\n",
        "\n",
        "        temp_cols = [col, \"log_\"+col, \"sqrt_\"+col, \"bx_cx_\"+col, \"y_J_\"+col, \"log_sqrt\"+col, \"pow_\"+col, \"pow2_\"+col]\n",
        "\n",
        "        train_copy[temp_cols] = train_copy[temp_cols].fillna(0)\n",
        "        test_copy[temp_cols] = test_copy[temp_cols].fillna(0)\n",
        "\n",
        "        pca = TruncatedSVD(n_components=1)\n",
        "        x_pca_train = pca.fit_transform(train_copy[temp_cols])\n",
        "        x_pca_test = pca.transform(test_copy[temp_cols])\n",
        "        x_pca_train = pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb\"])\n",
        "        x_pca_test = pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb\"])\n",
        "        temp_cols.append(col+\"_pca_comb\")\n",
        "\n",
        "        test_copy = test_copy.reset_index(drop=True)\n",
        "\n",
        "        train_copy = pd.concat([train_copy, x_pca_train], axis='columns')\n",
        "        test_copy = pd.concat([test_copy, x_pca_test], axis='columns')\n",
        "\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        ll_scores = []\n",
        "\n",
        "        for f in temp_cols:\n",
        "            X = train_copy[[f]].values\n",
        "            y = train_copy[target].values\n",
        "\n",
        "            log_loss_score = []\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train, y_train = X[train_idx], y[train_idx]\n",
        "                x_val, y_val = X[val_idx], y[val_idx]\n",
        "#                 model =   SVC(gamma=\"auto\", probability=True, random_state=42)\n",
        "                model =   CatBoostClassifier(**cat_params,verbose=False)\n",
        "                model= lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "                model.fit(X_train, encode(y_train, target_map))\n",
        "                y_pred = model.predict_proba(x_val)\n",
        "#                 print(log_loss(encode(y_val, target_map),y_pred))\n",
        "                log_loss_score.append(log_loss(encode(y_val, target_map),y_pred))\n",
        "            ll_scores.append((f, np.mean(log_loss_score)))\n",
        "\n",
        "            if overall_best_score > np.mean(log_loss_score):\n",
        "                overall_best_score = np.mean(log_loss_score)\n",
        "                overall_best_col = log_loss_score\n",
        "\n",
        "            if f == col:\n",
        "                orig_mae = np.mean(log_loss_score)\n",
        "\n",
        "        best_col, best_loss = sorted(ll_scores, key=lambda x: x[1], reverse=False)[0]\n",
        "        cols_to_drop = [f for f in temp_cols if f != best_col]\n",
        "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
        "\n",
        "        if cols_to_drop:\n",
        "            unimportant_features = unimportant_features+cols_to_drop\n",
        "        table.add_row([col,orig_mae,best_col ,best_loss])\n",
        "    print(table)\n",
        "    print(\"overall best CV Log Loss score: \",overall_best_score)\n",
        "    return train_copy, test_copy\n",
        "\n",
        "train, test= transformer(train, test,cont_cols, target)"
      ],
      "metadata": {
        "id": "CJ1yAiwytRG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = ['Clustered Feature', 'Log Loss (CV-TRAIN)']\n",
        "for col in cont_cols:\n",
        "    sub_set=[f for f in unimportant_features if col in f]\n",
        "    temp_train=train[sub_set]\n",
        "    temp_test=test[sub_set]\n",
        "    sc=StandardScaler()\n",
        "    temp_train=sc.fit_transform(temp_train)\n",
        "    temp_test=sc.transform(temp_test)\n",
        "    model = KMeans()\n",
        "\n",
        "    # print(ideal_clusters)\n",
        "    kmeans = KMeans(n_clusters=25)\n",
        "    kmeans.fit(np.array(temp_train))\n",
        "    labels_train = kmeans.labels_\n",
        "\n",
        "    train[col+\"_unimp_cluster_WOE\"] = labels_train\n",
        "    test[col+\"_unimp_cluster_WOE\"] = kmeans.predict(np.array(temp_test))\n",
        "\n",
        "\n",
        "    kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    X=train[[col+\"_unimp_cluster_WOE\"]].values\n",
        "    y=train[target].values\n",
        "\n",
        "    log_loss_score=[]\n",
        "    for train_idx, val_idx in kf.split(X,y):\n",
        "        X_train,y_train=X[train_idx],y[train_idx]\n",
        "        x_val,y_val=X[val_idx],y[val_idx]\n",
        "        model =  lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "        model.fit(X_train, encode(y_train, target_map))\n",
        "        y_pred = model.predict_proba(x_val)\n",
        "        log_loss_score.append(log_loss(encode(y_val, target_map),y_pred))\n",
        "\n",
        "\n",
        "    table.add_row([col+\"_unimp_cluster_WOE\",np.mean(log_loss_score)])\n",
        "    if overall_best_score>np.mean(log_loss_score):\n",
        "        overall_best_score=np.mean(log_loss_score)\n",
        "        overall_best_col=col+\"_unimp_cluster_WOE\"\n",
        "\n",
        "print(table)\n",
        "print(\"overall best CV score: \", overall_best_score)"
      ],
      "metadata": {
        "id": "QiTEDht1rbD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_num = [f for f in test.columns if (test[f].isna().sum() > 0).any()]\n",
        "train,test = fill_missing_numerical(train,test,target,missing_num,3)"
      ],
      "metadata": {
        "id": "ufDkkSw2rkDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def better_features(train, test, target, cols, best_score):\n",
        "    new_cols = []\n",
        "    skf = KFold(n_splits=5, shuffle=True, random_state=42)  # Stratified k-fold object\n",
        "    best_list=[]\n",
        "    for i in tqdm(range(len(cols)), desc='Generating Columns'):\n",
        "        col1 = cols[i]\n",
        "        temp_df = pd.DataFrame()  # Temporary dataframe to store the generated columns\n",
        "        temp_df_test = pd.DataFrame()  # Temporary dataframe for test data\n",
        "\n",
        "        for j in range(i+1, len(cols)):\n",
        "            col2 = cols[j]\n",
        "            # Multiply\n",
        "            temp_df[col1 + '*' + col2] = train[col1] * train[col2]\n",
        "            temp_df_test[col1 + '*' + col2] = test[col1] * test[col2]\n",
        "\n",
        "            # Divide (col1 / col2)\n",
        "            temp_df[col1 + '/' + col2] = train[col1] / (train[col2] + 1e-5)\n",
        "            temp_df_test[col1 + '/' + col2] = test[col1] / (test[col2] + 1e-5)\n",
        "\n",
        "            # Divide (col2 / col1)\n",
        "            temp_df[col2 + '/' + col1] = train[col2] / (train[col1] + 1e-5)\n",
        "            temp_df_test[col2 + '/' + col1] = test[col2] / (test[col1] + 1e-5)\n",
        "\n",
        "            # Subtract\n",
        "            temp_df[col1 + '-' + col2] = train[col1] - train[col2]\n",
        "            temp_df_test[col1 + '-' + col2] = test[col1] - test[col2]\n",
        "\n",
        "            # Add\n",
        "            temp_df[col1 + '+' + col2] = train[col1] + train[col2]\n",
        "            temp_df_test[col1 + '+' + col2] = test[col1] + test[col2]\n",
        "\n",
        "        SCORES = []\n",
        "        for column in temp_df.columns:\n",
        "            scores = []\n",
        "            for train_index, val_index in skf.split(train, train[target]):\n",
        "                X_train, X_val = temp_df[column].iloc[train_index].values.reshape(-1, 1), temp_df[column].iloc[val_index].values.reshape(-1, 1)\n",
        "                y_train, y_val = train[target].iloc[train_index], train[target].iloc[val_index]\n",
        "                model = lgb.LGBMClassifier(**lgb_params)\n",
        "                model.fit(X_train, encode(y_train, target_map))\n",
        "                y_pred = model.predict_proba(X_val)\n",
        "                scores.append(log_loss(encode(y_val, target_map),y_pred))\n",
        "            mean_score = np.mean(scores)\n",
        "            SCORES.append((column, mean_score))\n",
        "\n",
        "        if SCORES:\n",
        "            best_col, best_loss = sorted(SCORES, key=lambda x: x[1],reverse=False)[0]\n",
        "            corr_with_other_cols = train.drop([target] + new_cols, axis=1).corrwith(temp_df[best_col])\n",
        "            if (corr_with_other_cols.abs().max() < 0.9 or best_loss < best_score) and corr_with_other_cols.abs().max() !=1 :\n",
        "                train[best_col] = temp_df[best_col]\n",
        "                test[best_col] = temp_df_test[best_col]\n",
        "                new_cols.append(best_col)\n",
        "                print(f\"Added column '{best_col}' with Log Loss Score: {best_loss:.4f} & Correlation {corr_with_other_cols.abs().max():.4f}\")\n",
        "\n",
        "    return train, test, new_cols"
      ],
      "metadata": {
        "id": "AAulpqmWrp9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cols=['Bilirubin*Prothrombin_target',\n",
        " 'Albumin/Bilirubin_target',\n",
        " 'Copper+Bilirubin_target',\n",
        " 'SGOT+Bilirubin_target',\n",
        " 'Prothrombin*Bilirubin_target',\n",
        " 'bx_cx_N_Days*Bilirubin_target',\n",
        " 'pow2_Age*Bilirubin_target',\n",
        " 'log_Cholesterol+Bilirubin_target',\n",
        " 'Alk_Phos_pca_comb+Bilirubin_target',\n",
        " 'bx_cx_Tryglicerides-Bilirubin_target',\n",
        " 'bx_cx_Platelets-Bilirubin_target',\n",
        " 'Bilirubin_target/Edema_target',\n",
        " 'Edema_count-Bilirubin_target',\n",
        " 'Bilirubin_target/Edema_count_label',\n",
        " 'Bilirubin_target*Prothrombin_target',\n",
        " 'Prothrombin_target/Bilirubin_count',\n",
        " 'Bilirubin_count_label/Stage_target',\n",
        " 'Albumin_target*Prothrombin_target',\n",
        " 'Stage_target/Albumin_count',\n",
        " 'Prothrombin_target/Albumin_count_label',\n",
        " 'Stage_target/Prothrombin_count',\n",
        " 'Prothrombin_count_label/Stage_target',\n",
        " 'Stage_target/Copper_unimp_cluster_WOE',\n",
        " 'N_Days_unimp_cluster_WOE/Copper_unimp_cluster_WOE',\n",
        " 'Copper_unimp_cluster_WOE/Age_unimp_cluster_WOE',\n",
        " 'Cholesterol_unimp_cluster_WOE/Copper_unimp_cluster_WOE',\n",
        " 'SGOT_unimp_cluster_WOE/Copper_unimp_cluster_WOE',\n",
        " 'Alk_Phos_unimp_cluster_WOE/SGOT_unimp_cluster_WOE',\n",
        " 'SGOT_unimp_cluster_WOE/Tryglicerides_unimp_cluster_WOE',\n",
        " 'Platelets_unimp_cluster_WOE/Tryglicerides_unimp_cluster_WOE']"
      ],
      "metadata": {
        "id": "FKU3xUBSrtD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_arithmetic_operations(train_df, test_df, expressions_list):\n",
        "    for expression in expressions_list:\n",
        "        if expression not in train_df.columns:\n",
        "            # Split the expression based on operators (+, -, *, /)\n",
        "            parts = expression.split('+') if '+' in expression else \\\n",
        "                    expression.split('-') if '-' in expression else \\\n",
        "                    expression.split('*') if '*' in expression else \\\n",
        "                    expression.split('/')\n",
        "\n",
        "            # Get the DataFrame column names involved in the operation\n",
        "            cols = [col for col in parts]\n",
        "\n",
        "            # Perform the corresponding arithmetic operation based on the operator in the expression\n",
        "            if cols[0] in train_df.columns and cols[1] in train_df.columns:\n",
        "                if '+' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] + train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] + test_df[cols[1]]\n",
        "                elif '-' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] - train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] - test_df[cols[1]]\n",
        "                elif '*' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] * train_df[cols[1]]\n",
        "                    test_df[expression] = test_df[cols[0]] * test_df[cols[1]]\n",
        "                elif '/' in expression:\n",
        "                    train_df[expression] = train_df[cols[0]] / (train_df[cols[1]]+1e-5)\n",
        "                    test_df[expression] = test_df[cols[0]] /( test_df[cols[1]]+1e-5)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train, test = apply_arithmetic_operations(train, test, new_cols)"
      ],
      "metadata": {
        "id": "ltH2sSWJrtn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_drop=[ f for f in unimportant_features if f in train.columns]\n",
        "train=train.drop(columns=first_drop)\n",
        "test=test.drop(columns=first_drop)"
      ],
      "metadata": {
        "id": "EnuwLlqerw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_drop_list=[]\n",
        "\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = ['Original', 'Final Transformation', 'Log Loss CV']\n",
        "threshold=0.95\n",
        "# It is possible that multiple parent features share same child features, so store selected features to avoid selecting the same feature again\n",
        "best_cols=[]\n",
        "\n",
        "for col in cont_cols:\n",
        "    sub_set=[f for f in train.columns if (str(col) in str(f)) and (train[f].nunique()>2)]\n",
        "#     print(sub_set)\n",
        "    if len(sub_set)>2:\n",
        "        correlated_features = []\n",
        "\n",
        "        for i, feature in enumerate(sub_set):\n",
        "            # Check correlation with all remaining features\n",
        "            for j in range(i+1, len(sub_set)):\n",
        "                correlation = np.abs(train[feature].corr(train[sub_set[j]]))\n",
        "                # If correlation is greater than threshold, add to list of highly correlated features\n",
        "                if correlation > threshold:\n",
        "                    correlated_features.append(sub_set[j])\n",
        "\n",
        "        # Remove duplicate features from the list\n",
        "        correlated_features = list(set(correlated_features))\n",
        "#         print(correlated_features)\n",
        "        if len(correlated_features)>=2:\n",
        "\n",
        "            temp_train=train[correlated_features]\n",
        "            temp_test=test[correlated_features]\n",
        "            #Scale before applying PCA\n",
        "            sc=StandardScaler()\n",
        "            temp_train=sc.fit_transform(temp_train)\n",
        "            temp_test=sc.transform(temp_test)\n",
        "\n",
        "            # Initiate PCA\n",
        "            pca=TruncatedSVD(n_components=1)\n",
        "            x_pca_train=pca.fit_transform(temp_train)\n",
        "            x_pca_test=pca.transform(temp_test)\n",
        "            x_pca_train=pd.DataFrame(x_pca_train, columns=[col+\"_pca_comb_final\"])\n",
        "            x_pca_test=pd.DataFrame(x_pca_test, columns=[col+\"_pca_comb_final\"])\n",
        "            train=pd.concat([train,x_pca_train],axis='columns')\n",
        "            test=pd.concat([test,x_pca_test],axis='columns')\n",
        "\n",
        "            # Clustering\n",
        "            model = KMeans()\n",
        "            kmeans = KMeans(n_clusters=10)\n",
        "            kmeans.fit(np.array(temp_train))\n",
        "            labels_train = kmeans.labels_\n",
        "\n",
        "            train[col+'_final_cluster'] = labels_train\n",
        "            test[col+'_final_cluster'] = kmeans.predict(np.array(temp_test))\n",
        "\n",
        "\n",
        "            correlated_features=correlated_features+[col+\"_pca_comb_final\",col+\"_final_cluster\"]\n",
        "\n",
        "            # See which transformation along with the original is giving you the best univariate fit with target\n",
        "            kf=KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "            scores=[]\n",
        "\n",
        "            for f in correlated_features:\n",
        "                X=train[[f]].values\n",
        "                y=train[target].values\n",
        "\n",
        "                log_loss_score=[]\n",
        "                for train_idx, val_idx in kf.split(X,y):\n",
        "                    X_train,y_train=X[train_idx],y[train_idx]\n",
        "                    X_val,y_val=X[val_idx],y[val_idx]\n",
        "                    model = lgb.LGBMClassifier(**lgb_params)\n",
        "                    model.fit(X_train, encode(y_train, target_map))\n",
        "                    y_pred = model.predict_proba(x_val)\n",
        "                    log_loss_score.append(log_loss(encode(y_val, target_map),y_pred))\n",
        "                if f not in best_cols:\n",
        "                    scores.append((f,np.mean(log_loss_score)))\n",
        "            best_col, best_loss=sorted(scores, key=lambda x:x[1], reverse=False)[0]\n",
        "            best_cols.append(best_col)\n",
        "\n",
        "            cols_to_drop = [f for f in correlated_features if  f not in best_cols]\n",
        "            if cols_to_drop:\n",
        "                final_drop_list=final_drop_list+cols_to_drop\n",
        "            table.add_row([col,best_col ,best_loss])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "iuJOTs7-r2-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_drop_list=[*set(final_drop_list)]\n",
        "train=train.drop(columns=final_drop_list)\n",
        "test=test.drop(columns=final_drop_list)"
      ],
      "metadata": {
        "id": "EDhq83s3r4aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_features=[f for f in train.columns if f not in [target]]\n",
        "final_features=[*set(final_features)]\n",
        "\n",
        "sc=StandardScaler()\n",
        "\n",
        "train_scaled=train.copy()\n",
        "test_scaled=test.copy()\n",
        "train_scaled[final_features]=sc.fit_transform(train[final_features])\n",
        "test_scaled[final_features]=sc.transform(test[final_features])"
      ],
      "metadata": {
        "id": "dntB038or5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_processor(train, test):\n",
        "    cols=train.drop(columns=[target]).columns\n",
        "    train_cop=train.copy()\n",
        "    test_cop=test.copy()\n",
        "    drop_cols=[]\n",
        "    for i, feature in enumerate(cols):\n",
        "        for j in range(i+1, len(cols)):\n",
        "            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n",
        "                if cols[j] not in drop_cols:\n",
        "                    drop_cols.append(cols[j])\n",
        "    print(drop_cols)\n",
        "    train_cop.drop(columns=drop_cols,inplace=True)\n",
        "    test_cop.drop(columns=drop_cols,inplace=True)\n",
        "\n",
        "    return train_cop, test_cop\n",
        "\n",
        "\n",
        "train_cop, test_cop = post_processor(train_scaled, test_scaled)"
      ],
      "metadata": {
        "id": "2GjN_d_4r7qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_cop.drop(columns=[target])\n",
        "y_train = train[target]\n",
        "\n",
        "X_test = test_cop.copy()\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "CstpE4M4r9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_important_features(X_train, y_train, n,model_input):\n",
        "    xgb_params = {\n",
        "            'n_estimators': 200,\n",
        "            'learning_rate': 0.05,\n",
        "            'max_depth': 4,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.1,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'mlogloss',\n",
        "            'objective': 'multi:softprob',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    lgb_params = {\n",
        "            'n_estimators': 200,\n",
        "            'max_depth': 7,\n",
        "            'learning_rate': 0.05,\n",
        "            'subsample': 0.20,\n",
        "            'colsample_bytree': 0.56,\n",
        "            'reg_alpha': 0.25,\n",
        "            'reg_lambda': 5e-08,\n",
        "            'objective': 'multiclass',\n",
        "            'metric': 'multi_logloss',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    cb_params = {\n",
        "            'iterations': 200,\n",
        "            'depth': 7,\n",
        "            'learning_rate': 0.1,\n",
        "            'l2_leaf_reg': 0.7,\n",
        "            'random_strength': 0.2,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'one_hot_max_size': 70,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'MultiClass',\n",
        "            'loss_function': 'MultiClass',\n",
        "            'random_state': 42,\n",
        "        }\n",
        "    if 'xgb' in model_input:\n",
        "        model = xgb.XGBClassifier(**xgb_params)\n",
        "    elif 'cat' in model_input:\n",
        "        model=CatBoostClassifier(**cb_params)\n",
        "    else:\n",
        "        model=lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "    feature_importances_list = []\n",
        "\n",
        "    for train_idx, val_idx in kfold.split(X_train):\n",
        "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_train_fold, encode(y_train_fold,target_map), verbose=False)\n",
        "\n",
        "        y_pred = model.predict(X_val_fold)\n",
        "        f1_scores.append(f1_score(encode(y_val_fold,target_map), y_pred, average='micro'))\n",
        "        feature_importances = model.feature_importances_\n",
        "        feature_importances_list.append(feature_importances)\n",
        "\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "    avg_feature_importances = np.mean(feature_importances_list, axis=0)\n",
        "\n",
        "    feature_importance_list = [(X_train.columns[i], importance) for i, importance in enumerate(avg_feature_importances)]\n",
        "    sorted_features = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
        "    top_n_features = [feature[0] for feature in sorted_features[:n]]\n",
        "\n",
        "    display_features=top_n_features[:25]\n",
        "\n",
        "    sns.set_palette([(0.8, 0.56, 0.65)])\n",
        "    plt.figure(figsize=(8, 15))\n",
        "    plt.barh(range(len(display_features)), [avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features])\n",
        "    plt.yticks(range(len(display_features)), display_features, fontsize=12)\n",
        "    plt.xlabel('Average Feature Importance', fontsize=14)\n",
        "    plt.ylabel('Features', fontsize=10)\n",
        "    plt.title(f'Top {25} of {n} Feature Importances with best F1 score {avg_f1}', fontsize=16)\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
        "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    plt.xticks(fontsize=8)\n",
        "    plt.yticks(fontsize=8)\n",
        "\n",
        "    # Add data labels on the bars\n",
        "    for index, value in enumerate([avg_feature_importances[X_train.columns.get_loc(feature)] for feature in display_features]):\n",
        "        plt.text(value + 0.005, index, f'{value:.3f}', fontsize=12, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return top_n_features"
      ],
      "metadata": {
        "id": "hUX0pD48sC5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_imp_features_cat=get_most_important_features(X_train.reset_index(drop=True), y_train,45, 'cat')\n",
        "n_imp_features_xgb=get_most_important_features(X_train.reset_index(drop=True), y_train,45, 'xgb')\n",
        "n_imp_features_lgbm=get_most_important_features(X_train.reset_index(drop=True), y_train,45, 'lgbm')"
      ],
      "metadata": {
        "id": "GFkaaBQqsEXU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}